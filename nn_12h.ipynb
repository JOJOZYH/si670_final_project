{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load your dataset\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Impute missing values using interpolation\n",
        "data.interpolate(method='linear', limit_direction='forward', inplace=True)\n",
        "\n",
        "# Create a datetime column\n",
        "data['datetime'] = pd.to_datetime({\n",
        "    'year': data['year'],  # Used to create datetime, will be dropped afterward\n",
        "    'month': data['month'],\n",
        "    'day': data['day'],\n",
        "    'hour': data['hour']\n",
        "})\n",
        "\n",
        "# Set 'datetime' as the index\n",
        "data.set_index('datetime', inplace=True)\n",
        "\n",
        "# Drop the 'year' column as per your requirement\n",
        "data.drop(['year'], axis=1, inplace=True)\n",
        "\n",
        "# Extract day of the week and weekend indicator\n",
        "data['day_of_week'] = data.index.dayofweek\n",
        "data['is_weekend'] = data['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
        "\n",
        "# One-hot encode 'cbwd'\n",
        "data = pd.get_dummies(data, columns=['cbwd'])\n",
        "\n",
        "# Define the number of hours ahead to predict\n",
        "n_hours_ahead = 12  # Change this value to predict different hours ahead\n",
        "\n",
        "# Shift the 'pm2.5' column to get future values\n",
        "data['pm2.5_future'] = data['pm2.5'].shift(-n_hours_ahead)\n",
        "\n",
        "# Drop rows with NaN values resulting from shifting\n",
        "data.dropna(subset=['pm2.5_future'], inplace=True)\n",
        "\n",
        "# Create the binary target variable based on 'pm2.5_future'\n",
        "threshold = 50\n",
        "data['pm2.5_binary'] = data['pm2.5_future'].apply(lambda x: 1 if x > threshold else 0)\n",
        "\n",
        "# List to hold DataFrames of lag features\n",
        "lagged_features = []\n",
        "\n",
        "# Define the features and lags\n",
        "lag_features = ['DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir']\n",
        "lags = range(1, 25)  # Lags from 1 to 24 hours\n",
        "\n",
        "# Generate lagged features for specified features\n",
        "for feature in lag_features + ['pm2.5']:\n",
        "    # Create a dictionary of lagged series\n",
        "    lagged_data = {f'{feature}_lag_{lag}': data[feature].shift(lag) for lag in lags}\n",
        "    # Create a DataFrame from the dictionary\n",
        "    lagged_df = pd.DataFrame(lagged_data)\n",
        "    # Append to the list\n",
        "    lagged_features.append(lagged_df)\n",
        "\n",
        "# Concatenate all lagged features into a single DataFrame\n",
        "lagged_features_df = pd.concat(lagged_features, axis=1)\n",
        "\n",
        "# Concatenate lagged features with the original data\n",
        "data = pd.concat([data, lagged_features_df], axis=1)\n",
        "\n",
        "# Drop rows with NaN values resulting from lagging\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Sort data by datetime\n",
        "data.sort_index(inplace=True)\n",
        "\n",
        "# Define the split point (e.g., last 20% of data for testing)\n",
        "split_fraction = 0.8\n",
        "split_point = int(len(data) * split_fraction)\n",
        "\n",
        "# Split the data\n",
        "train_data = data.iloc[:split_point]\n",
        "test_data = data.iloc[split_point:]\n",
        "\n",
        "# Separate features and target\n",
        "X_train = train_data.drop(['pm2.5', 'pm2.5_future', 'pm2.5_binary'], axis=1)\n",
        "y_train = train_data['pm2.5_binary']\n",
        "\n",
        "X_test = test_data.drop(['pm2.5', 'pm2.5_future', 'pm2.5_binary'], axis=1)\n",
        "y_test = test_data['pm2.5_binary']\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Identify numeric features to scale\n",
        "numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data\n",
        "X_train_scaled = X_train.copy()\n",
        "X_train_scaled[numeric_features] = scaler.fit_transform(X_train[numeric_features])\n",
        "\n",
        "# Apply the scaler to the test data\n",
        "X_test_scaled = X_test.copy()\n",
        "X_test_scaled[numeric_features] = scaler.transform(X_test[numeric_features])\n",
        "\n",
        "# Check class distribution in the training set\n",
        "from collections import Counter\n",
        "\n",
        "counter = Counter(y_train)\n",
        "print(f'Class distribution in training set: {counter}')\n",
        "\n",
        "# Calculate class weights if imbalance exists\n",
        "neg, pos = np.bincount(y_train)\n",
        "total = neg + pos\n",
        "class_weight = {0: (1 / neg) * (total / 2.0),\n",
        "                1: (1 / pos) * (total / 2.0)}\n",
        "\n",
        "print(f'Calculated class weights: {class_weight}')\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['Precision', 'Recall', 'AUC'])\n",
        "\n",
        "# Define early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    validation_split=0.2,  # Further split of training data for validation\n",
        "    shuffle=False,\n",
        "    epochs=100,\n",
        "    batch_size=64,\n",
        "    callbacks=[early_stopping],\n",
        "    class_weight=class_weight  # Use class weights if imbalance exists\n",
        ")\n",
        "\n",
        "# Predict probabilities on the test set\n",
        "y_pred_prob = model.predict(X_test_scaled)\n",
        "\n",
        "# Convert probabilities to binary predictions\n",
        "y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
        "\n",
        "# Evaluate the model\n",
        "from sklearn.metrics import f1_score, roc_auc_score, classification_report, confusion_matrix\n",
        "\n",
        "# F1 Score\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f'F1 Score: {f1:.4f}')\n",
        "\n",
        "# AUC-ROC\n",
        "auc = roc_auc_score(y_test, y_pred_prob)\n",
        "print(f'AUC-ROC: {auc:.4f}')\n",
        "\n",
        "# Classification Report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPI8YaCsP3f4",
        "outputId": "bb25799f-fd44-44fc-ce24-a90054ac0b5e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-7de5f46f33e4>:8: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
            "  data.interpolate(method='linear', limit_direction='forward', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution in training set: Counter({1: 21731, 0: 13280})\n",
            "Calculated class weights: {0: 1.3181852409638555, 1: 0.8055542772997101}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/data_adapter_utils.py:126: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  sample_weight[i] = class_weight.get(int(y[i]), 1.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - AUC: 0.6677 - Precision: 0.7228 - Recall: 0.6434 - loss: 0.6795 - val_AUC: 0.7389 - val_Precision: 0.7687 - val_Recall: 0.7007 - val_loss: 0.6181\n",
            "Epoch 2/100\n",
            "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.7515 - Precision: 0.7800 - Recall: 0.7021 - loss: 0.6079 - val_AUC: 0.7659 - val_Precision: 0.7852 - val_Recall: 0.6965 - val_loss: 0.5977\n",
            "Epoch 3/100\n",
            "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7847 - Precision: 0.8055 - Recall: 0.7170 - loss: 0.5616 - val_AUC: 0.7772 - val_Precision: 0.7960 - val_Recall: 0.6953 - val_loss: 0.5909\n",
            "Epoch 4/100\n",
            "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7895 - Precision: 0.8116 - Recall: 0.7176 - loss: 0.5573 - val_AUC: 0.7817 - val_Precision: 0.7974 - val_Recall: 0.6958 - val_loss: 0.5829\n",
            "Epoch 5/100\n",
            "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7974 - Precision: 0.8169 - Recall: 0.7208 - loss: 0.5485 - val_AUC: 0.7839 - val_Precision: 0.7857 - val_Recall: 0.7226 - val_loss: 0.5723\n",
            "Epoch 6/100\n",
            "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8057 - Precision: 0.8182 - Recall: 0.7309 - loss: 0.5379 - val_AUC: 0.7879 - val_Precision: 0.7936 - val_Recall: 0.7077 - val_loss: 0.5741\n",
            "Epoch 7/100\n",
            "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.8104 - Precision: 0.8225 - Recall: 0.7358 - loss: 0.5327 - val_AUC: 0.7864 - val_Precision: 0.7969 - val_Recall: 0.7084 - val_loss: 0.5748\n",
            "Epoch 8/100\n",
            "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.8174 - Precision: 0.8270 - Recall: 0.7422 - loss: 0.5232 - val_AUC: 0.7891 - val_Precision: 0.7993 - val_Recall: 0.7042 - val_loss: 0.5720\n",
            "Epoch 9/100\n",
            "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8189 - Precision: 0.8286 - Recall: 0.7474 - loss: 0.5217 - val_AUC: 0.7870 - val_Precision: 0.7940 - val_Recall: 0.7011 - val_loss: 0.5734\n",
            "Epoch 10/100\n",
            "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - AUC: 0.8255 - Precision: 0.8306 - Recall: 0.7490 - loss: 0.5132 - val_AUC: 0.7885 - val_Precision: 0.7926 - val_Recall: 0.7114 - val_loss: 0.5733\n",
            "Epoch 11/100\n",
            "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8319 - Precision: 0.8355 - Recall: 0.7619 - loss: 0.5046 - val_AUC: 0.7864 - val_Precision: 0.7881 - val_Recall: 0.7174 - val_loss: 0.5738\n",
            "Epoch 12/100\n",
            "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8325 - Precision: 0.8324 - Recall: 0.7636 - loss: 0.5028 - val_AUC: 0.7876 - val_Precision: 0.7842 - val_Recall: 0.7338 - val_loss: 0.5666\n",
            "Epoch 13/100\n",
            "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8388 - Precision: 0.8410 - Recall: 0.7733 - loss: 0.4944 - val_AUC: 0.7855 - val_Precision: 0.7799 - val_Recall: 0.7172 - val_loss: 0.5692\n",
            "Epoch 14/100\n",
            "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8458 - Precision: 0.8454 - Recall: 0.7850 - loss: 0.4844 - val_AUC: 0.7811 - val_Precision: 0.7834 - val_Recall: 0.7270 - val_loss: 0.5783\n",
            "Epoch 15/100\n",
            "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8449 - Precision: 0.8395 - Recall: 0.7873 - loss: 0.4858 - val_AUC: 0.7757 - val_Precision: 0.7897 - val_Recall: 0.6988 - val_loss: 0.5990\n",
            "Epoch 16/100\n",
            "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.8520 - Precision: 0.8439 - Recall: 0.7861 - loss: 0.4760 - val_AUC: 0.7778 - val_Precision: 0.7826 - val_Recall: 0.7093 - val_loss: 0.5930\n",
            "Epoch 17/100\n",
            "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.8577 - Precision: 0.8473 - Recall: 0.7987 - loss: 0.4668 - val_AUC: 0.7759 - val_Precision: 0.7820 - val_Recall: 0.7293 - val_loss: 0.6068\n",
            "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "F1 Score: 0.7959\n",
            "AUC-ROC: 0.8019\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.66      0.67      3348\n",
            "           1       0.79      0.80      0.80      5405\n",
            "\n",
            "    accuracy                           0.75      8753\n",
            "   macro avg       0.73      0.73      0.73      8753\n",
            "weighted avg       0.75      0.75      0.75      8753\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2207 1141]\n",
            " [1078 4327]]\n"
          ]
        }
      ]
    }
  ]
}