{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlpgUYlqgZsW",
        "outputId": "f0780790-a922-4405-f6c5-55a53bc1170c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No          0\n",
            "year        0\n",
            "month       0\n",
            "day         0\n",
            "hour        0\n",
            "pm2.5    2067\n",
            "DEWP        0\n",
            "TEMP        0\n",
            "PRES        0\n",
            "cbwd        0\n",
            "Iws         0\n",
            "Is          0\n",
            "Ir          0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load your dataset\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Check for missing values\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Option 1: Drop rows with missing values\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Create a datetime column\n",
        "data['datetime'] = pd.to_datetime(data[['year', 'month', 'day', 'hour']])\n",
        "\n",
        "# Set as index if needed\n",
        "data.set_index('datetime', inplace=True)\n",
        "\n",
        "# Extract day of the week and weekend indicator\n",
        "data['day_of_week'] = data.index.dayofweek\n",
        "data['is_weekend'] = data['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
        "\n",
        "# One-hot encode 'cbwd'\n",
        "data = pd.get_dummies(data, columns=['cbwd'])\n",
        "\n",
        "# Define the threshold\n",
        "threshold = 50\n",
        "\n",
        "# Create the binary target variable\n",
        "data['pm2.5_binary'] = data['pm2.5'].apply(lambda x: 1 if x > threshold else 0)\n",
        "\n",
        "# Create lag features for PM2.5\n",
        "for lag in range(1, 25):  # Lags from 1 to 24 hours\n",
        "    data[f'pm2.5_lag_{lag}'] = data['pm2.5'].shift(lag)\n",
        "\n",
        "# Drop rows with NaN values resulting from lagging\n",
        "data.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort data by datetime\n",
        "data.sort_index(inplace=True)\n",
        "\n",
        "# Define the split point (e.g., last 20% of data for testing)\n",
        "split_fraction = 0.8\n",
        "split_point = int(len(data) * split_fraction)\n",
        "\n",
        "# Split the data\n",
        "train_data = data.iloc[:split_point]\n",
        "test_data = data.iloc[split_point:]\n",
        "\n",
        "# Separate features and target\n",
        "X_train = train_data.drop(['pm2.5', 'pm2.5_binary'], axis=1)\n",
        "y_train = train_data['pm2.5_binary']\n",
        "\n",
        "X_test = test_data.drop(['pm2.5', 'pm2.5_binary'], axis=1)\n",
        "y_test = test_data['pm2.5_binary']\n"
      ],
      "metadata": {
        "id": "oySgyx4_gkWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Select features to scale\n",
        "features_to_scale = ['DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir'] + [f'pm2.5_lag_{lag}' for lag in range(1, 25)]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data\n",
        "X_train_scaled = X_train.copy()\n",
        "X_train_scaled[features_to_scale] = scaler.fit_transform(X_train[features_to_scale])\n",
        "\n",
        "# Apply the scaler to the test data\n",
        "X_test_scaled = X_test.copy()\n",
        "X_test_scaled[features_to_scale] = scaler.transform(X_test[features_to_scale])\n"
      ],
      "metadata": {
        "id": "dPr_ICsjgnEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['Precision', 'Recall', 'AUC'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LE7X8UVhgrd5",
        "outputId": "cdef4ffd-c9ce-4dfe-c614-0a66c3d54876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check class distribution\n",
        "from collections import Counter\n",
        "\n",
        "counter = Counter(y_train)\n",
        "print(f'Class distribution in training set: {counter}')\n",
        "\n",
        "# Calculate class weights\n",
        "neg, pos = np.bincount(y_train)\n",
        "total = neg + pos\n",
        "class_weight = {0: (1 / neg) * (total / 2.0),\n",
        "                1: (1 / pos) * (total / 2.0)}\n",
        "\n",
        "print(f'Calculated class weights: {class_weight}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jc4T58OegxG7",
        "outputId": "6f706dc4-4686-46c5-e244-501598d55ed9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution in training set: Counter({1: 20640, 0: 12746})\n",
            "Calculated class weights: {0: 1.3096657774988232, 1: 0.8087693798449612}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    validation_split=0.2,  # Further split training data for validation\n",
        "    epochs=100,\n",
        "    batch_size=64,\n",
        "    callbacks=[early_stopping],\n",
        "    class_weight=class_weight  # Adjust if imbalance exists\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQkq2q60g0Mc",
        "outputId": "48640466-c4b5-4e05-8184-91393cc710b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/data_adapter_utils.py:126: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  sample_weight[i] = class_weight.get(int(y[i]), 1.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - AUC: 0.5563 - Precision: 0.6687 - Recall: 0.5567 - loss: 33.9475 - val_AUC: 0.5137 - val_Precision: 0.6088 - val_Recall: 1.0000 - val_loss: 8.0088\n",
            "Epoch 2/100\n",
            "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.6952 - Precision: 0.7639 - Recall: 0.6688 - loss: 5.1675 - val_AUC: 0.5000 - val_Precision: 0.6077 - val_Recall: 1.0000 - val_loss: 15.3131\n",
            "Epoch 3/100\n",
            "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - AUC: 0.7211 - Precision: 0.7892 - Recall: 0.7082 - loss: 5.1764 - val_AUC: 0.5000 - val_Precision: 0.6077 - val_Recall: 1.0000 - val_loss: 17.5398\n",
            "Epoch 4/100\n",
            "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7341 - Precision: 0.7949 - Recall: 0.7094 - loss: 5.5144 - val_AUC: 0.5577 - val_Precision: 0.9839 - val_Recall: 0.0902 - val_loss: 15.9202\n",
            "Epoch 5/100\n",
            "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7833 - Precision: 0.8332 - Recall: 0.7479 - loss: 3.8300 - val_AUC: 0.6617 - val_Precision: 0.9755 - val_Recall: 0.2452 - val_loss: 8.2641\n",
            "Epoch 6/100\n",
            "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7845 - Precision: 0.8348 - Recall: 0.7551 - loss: 4.6849 - val_AUC: 0.6375 - val_Precision: 0.6539 - val_Recall: 0.9963 - val_loss: 4.7876\n",
            "Epoch 7/100\n",
            "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8551 - Precision: 0.8760 - Recall: 0.8163 - loss: 2.0637 - val_AUC: 0.6777 - val_Precision: 0.9820 - val_Recall: 0.2817 - val_loss: 7.7220\n",
            "Epoch 8/100\n",
            "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7758 - Precision: 0.8318 - Recall: 0.7511 - loss: 4.8593 - val_AUC: 0.9218 - val_Precision: 0.9195 - val_Recall: 0.8356 - val_loss: 0.9363\n",
            "Epoch 9/100\n",
            "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8116 - Precision: 0.8527 - Recall: 0.7794 - loss: 4.0311 - val_AUC: 0.5175 - val_Precision: 0.9829 - val_Recall: 0.0283 - val_loss: 35.7330\n",
            "Epoch 10/100\n",
            "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8404 - Precision: 0.8785 - Recall: 0.8049 - loss: 2.9651 - val_AUC: 0.7057 - val_Precision: 0.9824 - val_Recall: 0.3443 - val_loss: 6.9525\n",
            "Epoch 11/100\n",
            "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.8387 - Precision: 0.8783 - Recall: 0.8037 - loss: 2.6905 - val_AUC: 0.7569 - val_Precision: 0.7028 - val_Recall: 0.9941 - val_loss: 2.9316\n",
            "Epoch 12/100\n",
            "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - AUC: 0.8358 - Precision: 0.8682 - Recall: 0.8060 - loss: 2.7084 - val_AUC: 0.8402 - val_Precision: 0.9730 - val_Recall: 0.5870 - val_loss: 2.6391\n",
            "Epoch 13/100\n",
            "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8330 - Precision: 0.8695 - Recall: 0.8028 - loss: 3.2398 - val_AUC: 0.9026 - val_Precision: 0.9585 - val_Recall: 0.7464 - val_loss: 1.3886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict probabilities\n",
        "y_pred_prob = model.predict(X_test_scaled)\n",
        "\n",
        "# Convert probabilities to binary predictions\n",
        "y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
        "\n",
        "from sklearn.metrics import f1_score, roc_auc_score, classification_report, confusion_matrix\n",
        "\n",
        "# F1 Score\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f'F1 Score: {f1:.4f}')\n",
        "\n",
        "# AUC-ROC\n",
        "auc = roc_auc_score(y_test, y_pred_prob)\n",
        "print(f'AUC-ROC: {auc:.4f}')\n",
        "\n",
        "# Classification Report\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print('Confusion Matrix:')\n",
        "print(cm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2Tq4-pSg8v3",
        "outputId": "52e7d3e0-0cfb-4195-94e2-204b60374be6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step\n",
            "F1 Score: 0.8404\n",
            "AUC-ROC: 0.9249\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.42      0.58      3199\n",
            "           1       0.73      0.99      0.84      5148\n",
            "\n",
            "    accuracy                           0.77      8347\n",
            "   macro avg       0.84      0.70      0.71      8347\n",
            "weighted avg       0.82      0.77      0.74      8347\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1329 1870]\n",
            " [  62 5086]]\n"
          ]
        }
      ]
    }
  ]
}