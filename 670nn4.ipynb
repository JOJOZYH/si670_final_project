{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vye9RKgPlE29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "122be7bd-3812-45cb-fc7a-bdd97bedf45a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-d3f6486b1174>:8: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
            "  data.interpolate(method='linear', limit_direction='forward', inplace=True)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load your dataset\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Impute missing values using interpolation\n",
        "data.interpolate(method='linear', limit_direction='forward', inplace=True)\n",
        "\n",
        "# Create a datetime column without the 'year' feature\n",
        "data['datetime'] = pd.to_datetime({\n",
        "    'year': data['year'],  # Used to create datetime, will be dropped afterward\n",
        "    'month': data['month'],\n",
        "    'day': data['day'],\n",
        "    'hour': data['hour']\n",
        "})\n",
        "\n",
        "# Set 'datetime' as the index\n",
        "data.set_index('datetime', inplace=True)\n",
        "\n",
        "# Drop the 'year' column as per your requirement\n",
        "data.drop(['year'], axis=1, inplace=True)\n",
        "\n",
        "# Extract day of the week and weekend indicator\n",
        "data['day_of_week'] = data.index.dayofweek\n",
        "data['is_weekend'] = data['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
        "\n",
        "# One-hot encode 'cbwd'\n",
        "data = pd.get_dummies(data, columns=['cbwd'])\n",
        "\n",
        "# Create the binary target variable\n",
        "threshold = 50\n",
        "data['pm2.5_binary'] = data['pm2.5'].apply(lambda x: 1 if x > threshold else 0)\n",
        "\n",
        "# List to hold DataFrames of lag features\n",
        "lagged_features = []\n",
        "\n",
        "# Define the features and lags\n",
        "lag_features = ['DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir']\n",
        "lags = range(1, 25)  # Lags from 1 to 24 hours\n",
        "\n",
        "# Generate lagged features for specified features\n",
        "for feature in lag_features + ['pm2.5']:\n",
        "    # Create a dictionary of lagged series\n",
        "    lagged_data = {f'{feature}_lag_{lag}': data[feature].shift(lag) for lag in lags}\n",
        "    # Create a DataFrame from the dictionary\n",
        "    lagged_df = pd.DataFrame(lagged_data)\n",
        "    # Append to the list\n",
        "    lagged_features.append(lagged_df)\n",
        "\n",
        "# Concatenate all lagged features into a single DataFrame\n",
        "lagged_features_df = pd.concat(lagged_features, axis=1)\n",
        "\n",
        "# Concatenate lagged features with the original data\n",
        "data = pd.concat([data, lagged_features_df], axis=1)\n",
        "\n",
        "# Drop rows with NaN values resulting from lagging\n",
        "data.dropna(inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort data by datetime\n",
        "data.sort_index(inplace=True)\n",
        "\n",
        "# Define the split point (e.g., last 20% of data for testing)\n",
        "split_fraction = 0.8\n",
        "split_point = int(len(data) * split_fraction)\n",
        "\n",
        "# Split the data\n",
        "train_data = data.iloc[:split_point]\n",
        "test_data = data.iloc[split_point:]\n",
        "\n",
        "# Separate features and target\n",
        "X_train = train_data.drop(['pm2.5', 'pm2.5_binary'], axis=1)\n",
        "y_train = train_data['pm2.5_binary']\n",
        "\n",
        "X_test = test_data.drop(['pm2.5', 'pm2.5_binary'], axis=1)\n",
        "y_test = test_data['pm2.5_binary']\n"
      ],
      "metadata": {
        "id": "uGJY4EWFlU3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Identify numeric features to scale\n",
        "numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data\n",
        "X_train_scaled = X_train.copy()\n",
        "X_train_scaled[numeric_features] = scaler.fit_transform(X_train[numeric_features])\n",
        "\n",
        "# Apply the scaler to the test data\n",
        "X_test_scaled = X_test.copy()\n",
        "X_test_scaled[numeric_features] = scaler.transform(X_test[numeric_features])\n"
      ],
      "metadata": {
        "id": "ctbsmrojlayL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check class distribution in the training set\n",
        "from collections import Counter\n",
        "\n",
        "counter = Counter(y_train)\n",
        "print(f'Class distribution in training set: {counter}')\n",
        "\n",
        "# Calculate class weights if imbalance exists\n",
        "neg, pos = np.bincount(y_train)\n",
        "total = neg + pos\n",
        "class_weight = {0: (1 / neg) * (total / 2.0),\n",
        "                1: (1 / pos) * (total / 2.0)}\n",
        "\n",
        "print(f'Calculated class weights: {class_weight}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJzLvovulcZp",
        "outputId": "360ac01a-3b31-4e21-a05d-fcecbc71363a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution in training set: Counter({1: 21740, 0: 13280})\n",
            "Calculated class weights: {0: 1.3185240963855422, 1: 0.8054277828886844}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['Precision', 'Recall', 'AUC'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AnVAM1dleBu",
        "outputId": "bd0ee4d0-bb11-4695-b6eb-ea917161db26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    validation_split=0.2,  # Further split of training data for validation\n",
        "    epochs=100,\n",
        "    batch_size=64,\n",
        "    callbacks=[early_stopping],\n",
        "    class_weight=class_weight  # Use class weights if imbalance exists\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXLNUeMmliV-",
        "outputId": "62805842-c39c-4105-fed4-bebac2d60e8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/data_adapter_utils.py:126: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  sample_weight[i] = class_weight.get(int(y[i]), 1.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - AUC: 0.9142 - Precision: 0.8968 - Recall: 0.8539 - loss: 0.3510 - val_AUC: 0.9635 - val_Precision: 0.9445 - val_Recall: 0.8759 - val_loss: 0.2541\n",
            "Epoch 2/100\n",
            "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.9778 - Precision: 0.9542 - Recall: 0.9151 - loss: 0.1908 - val_AUC: 0.9691 - val_Precision: 0.9513 - val_Recall: 0.8957 - val_loss: 0.2422\n",
            "Epoch 3/100\n",
            "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9832 - Precision: 0.9620 - Recall: 0.9296 - loss: 0.1637 - val_AUC: 0.9724 - val_Precision: 0.9584 - val_Recall: 0.8796 - val_loss: 0.2416\n",
            "Epoch 4/100\n",
            "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.9854 - Precision: 0.9661 - Recall: 0.9345 - loss: 0.1530 - val_AUC: 0.9763 - val_Precision: 0.9561 - val_Recall: 0.9089 - val_loss: 0.2029\n",
            "Epoch 5/100\n",
            "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.9858 - Precision: 0.9643 - Recall: 0.9369 - loss: 0.1511 - val_AUC: 0.9739 - val_Precision: 0.9524 - val_Recall: 0.9092 - val_loss: 0.2179\n",
            "Epoch 6/100\n",
            "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - AUC: 0.9875 - Precision: 0.9683 - Recall: 0.9424 - loss: 0.1408 - val_AUC: 0.9728 - val_Precision: 0.9578 - val_Recall: 0.8873 - val_loss: 0.2275\n",
            "Epoch 7/100\n",
            "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9879 - Precision: 0.9684 - Recall: 0.9416 - loss: 0.1386 - val_AUC: 0.9769 - val_Precision: 0.9448 - val_Recall: 0.9329 - val_loss: 0.1895\n",
            "Epoch 8/100\n",
            "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9886 - Precision: 0.9694 - Recall: 0.9429 - loss: 0.1344 - val_AUC: 0.9719 - val_Precision: 0.9583 - val_Recall: 0.8826 - val_loss: 0.2528\n",
            "Epoch 9/100\n",
            "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9895 - Precision: 0.9715 - Recall: 0.9466 - loss: 0.1293 - val_AUC: 0.9735 - val_Precision: 0.9377 - val_Recall: 0.9392 - val_loss: 0.2080\n",
            "Epoch 10/100\n",
            "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9896 - Precision: 0.9689 - Recall: 0.9445 - loss: 0.1289 - val_AUC: 0.9733 - val_Precision: 0.9586 - val_Recall: 0.8961 - val_loss: 0.2265\n",
            "Epoch 11/100\n",
            "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - AUC: 0.9893 - Precision: 0.9700 - Recall: 0.9466 - loss: 0.1299 - val_AUC: 0.9757 - val_Precision: 0.9375 - val_Recall: 0.9369 - val_loss: 0.1937\n",
            "Epoch 12/100\n",
            "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.9907 - Precision: 0.9734 - Recall: 0.9502 - loss: 0.1207 - val_AUC: 0.9760 - val_Precision: 0.9658 - val_Recall: 0.8891 - val_loss: 0.2248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict probabilities on the test set\n",
        "y_pred_prob = model.predict(X_test_scaled)\n",
        "\n",
        "# Convert probabilities to binary predictions\n",
        "y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
        "\n",
        "# Evaluate the model\n",
        "from sklearn.metrics import f1_score, roc_auc_score, classification_report, confusion_matrix\n",
        "\n",
        "# F1 Score\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f'F1 Score: {f1:.4f}')\n",
        "\n",
        "# AUC-ROC\n",
        "auc = roc_auc_score(y_test, y_pred_prob)\n",
        "print(f'AUC-ROC: {auc:.4f}')\n",
        "\n",
        "# Classification Report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbQhxanRljnn",
        "outputId": "28094db1-e85c-48ea-93a9-cd0c28111ccc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "F1 Score: 0.9481\n",
            "AUC-ROC: 0.9844\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.93      0.92      3348\n",
            "           1       0.96      0.94      0.95      5408\n",
            "\n",
            "    accuracy                           0.94      8756\n",
            "   macro avg       0.93      0.94      0.93      8756\n",
            "weighted avg       0.94      0.94      0.94      8756\n",
            "\n",
            "Confusion Matrix:\n",
            "[[3115  233]\n",
            " [ 324 5084]]\n"
          ]
        }
      ]
    }
  ]
}